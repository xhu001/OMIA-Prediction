{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1moVejcnopr",
    "outputId": "973b32e3-9b1f-4fda-8cc4-b44e0419b2e8"
   },
   "outputs": [],
   "source": [
    "!pip install Biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# get unique species gene name (NCBI ID as well)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('./domestic_variants_missense_only_purge_3.csv')  # Replace 'your_file.csv' with the actual file path\n",
    "\n",
    "# Drop duplicates based on 'Species Name' and 'Gene', keeping the first occurrence\n",
    "unique_df = df.drop_duplicates(subset=['Species Name', 'Gene'], keep='first')\n",
    "\n",
    "# Select only the relevant columns\n",
    "result_df = unique_df[['Species Name', 'Gene', 'NCBI ID']]\n",
    "\n",
    "# Save or display the result\n",
    "print(result_df)\n",
    "result_df.to_csv('./unique_species_gene.csv', index=False)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "# use unique species gene name, get UniProt ID for Human and Animal\n",
    "\n",
    "# Function to query UniProt for a specific gene and organism\n",
    "def fetch_uniprot_id(gene, taxonomy_id):\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "    params = {\n",
    "        \"query\": f\"gene:{gene} AND taxonomy_id:{taxonomy_id}\",\n",
    "        \"format\": \"tsv\",\n",
    "        \"fields\": \"accession,gene_primary\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200 and response.text:\n",
    "        lines = response.text.strip().split(\"\\n\")\n",
    "        # Skip header and return the first result's ID\n",
    "        return lines[1].split(\"\\t\")[0] if len(lines) > 1 else \"Not Found\"\n",
    "    else:\n",
    "        return \"Not Found\"\n",
    "\n",
    "\n",
    "# Define a mapping dictionary for species name to taxonomy ID\n",
    "#animal_taxonomy_id = \"9986\"  # Sheep\"9940\" # Dog\"9615\" pig \"9823\"  cattle \"9913\"  cat \"9685\" horse \"9796\" rabbit \"9986\"\n",
    "taxonomy_mapping = {\n",
    "    'rabbit': \"9986\",\n",
    "    'pig': \"9823\",\n",
    "    'horse':\"9796\",\n",
    "    'cat':\"9685\",\n",
    "    'cattle':\"9913\",\n",
    "    'dog':\"9615\",\n",
    "    'sheep':\"9940\",\n",
    "    'goat':\"9925\",\n",
    "    # Add more species and their taxonomy IDs as needed\n",
    "}\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('./unique_species_gene.csv')  # \n",
    "\n",
    "\n",
    "\n",
    "# Add a new column 'animal_taxonomy_id' based on the mapping\n",
    "df['animal_taxonomy_id'] = df['Species Name'].map(taxonomy_mapping)\n",
    "\n",
    "# Save or display the result\n",
    "#print(df)\n",
    "df.to_csv('species_gene_animal_taxonomy_id.csv', index=False)  # Uncomment to save the result to a new CSV file\n",
    "\n",
    "\n",
    "# Fetch UniProt IDs for human\n",
    "# Drop duplicates based on 'Gene', this is for Human Uniprot\n",
    "temp_df = df[['Gene']]\n",
    "unique_human_gene_df = temp_df.drop_duplicates(subset=['Gene'])\n",
    "human_gene_list = unique_human_gene_df.Gene.tolist()\n",
    "human_taxonomy_id = \"9606\"  # Human\n",
    "human_uniprot_id_data = []\n",
    "\n",
    "for gene in human_gene_list:\n",
    "    human_id = fetch_uniprot_id(gene, human_taxonomy_id)\n",
    "    print(human_id)\n",
    "    human_uniprot_id_data.append({\"Gene\": gene, \"Human ID\": human_id})\n",
    "\n",
    "# Create a DataFrame and save as csv\n",
    "human_uniprot_id_df = pd.DataFrame(human_uniprot_id_data)\n",
    "print(human_uniprot_id_df)\n",
    "human_uniprot_id_df.to_csv('./human_uniprot_id.csv', index=False)  # \n",
    "\n",
    "\n",
    "# Fetch UniProt IDs for animals \n",
    "# Iterate through each row and extract the required columns\n",
    "animal_uniprot_id_data = []\n",
    "for index, row in df.iterrows():\n",
    "    gene = row['Gene']\n",
    "    ncbi_id = row['NCBI ID']\n",
    "    animal_taxonomy_id = row['animal_taxonomy_id']\n",
    "    animal = row['Species Name']\n",
    "    animal_id = fetch_uniprot_id(gene, animal_taxonomy_id)\n",
    "    print(animal_id)\n",
    "    animal_uniprot_id_data.append({\"Species Name\":animal,\"Gene\": gene, \"Animal ID\": animal_id})\n",
    "    \n",
    "# Create a DataFrame and save as csv\n",
    "animal_uniprot_id_df = pd.DataFrame(animal_uniprot_id_data)\n",
    "print(animal_uniprot_id_df)\n",
    "animal_uniprot_id_df.to_csv('./animal_uniprot_id.csv', index=False)  # \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge UniProt ID \n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into dataframes\n",
    "file1 = \"./domestic_variants_missense_only_purge_3.csv\"   \n",
    "file2 = \"./animal_uniprot_id.csv\"  \n",
    "file3 = \"./human_uniprot_id.csv\"  \n",
    "\n",
    "# Assuming tab-delimited files\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "df3 = pd.read_csv(file3)\n",
    "\n",
    "# Merge animal Uniprot ID\n",
    "merged_df = pd.merge(df1, df2, on=[\"Species Name\", \"Gene\"], how=\"inner\")\n",
    "#print(merged_df)\n",
    "\n",
    "# Merge Human Uniprot ID\n",
    "merged_df = pd.merge(merged_df, df3, on=[ \"Gene\"], how=\"inner\")\n",
    "\n",
    "print(merged_df)\n",
    "\n",
    "# Save the merged data to a new CSV\n",
    "output_file = \"Uniport_merged_file.csv\"\n",
    "merged_df.to_csv(output_file)\n",
    "\n",
    "print(f\"Merged file saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHvPUjq-xBJB",
    "outputId": "55a8e3a4-f517-4fe0-d5ad-0e7f935b6d3c"
   },
   "outputs": [],
   "source": [
    "# get protein sequence with NCBI and Uniport's ID\n",
    "from Bio import Entrez\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def fetch_and_save_protein_sequence_from_ncbi(protein_ids, output_folder):\n",
    "    # Email required by NCBI to use their API\n",
    "    Entrez.email = \"your_email@example.com\"\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # List to keep track of filenames for zipping\n",
    "    file_paths = []\n",
    "\n",
    "    # Iterate over protein IDs and fetch sequence in FASTA format\n",
    "    for protein_id in protein_ids:\n",
    "        \n",
    "        if protein_id != \"Not Found\" and protein_id != \"\":\n",
    "        \n",
    "            try:\n",
    "                # Fetch the record from NCBI using the Entrez module\n",
    "                handle = Entrez.efetch(db=\"protein\", id=protein_id, rettype=\"fasta\", retmode=\"text\")\n",
    "                sequence = handle.read()\n",
    "                handle.close()\n",
    "\n",
    "                # Define the file path\n",
    "                file_path = os.path.join(output_folder, f\"{protein_id}.txt\")\n",
    "                file_paths.append(file_path)\n",
    "\n",
    "                # Save the FASTA sequence to the file\n",
    "                with open(file_path, \"w\") as file:\n",
    "                    file.write(sequence)\n",
    "                print(f\"Saved FASTA sequence for {protein_id} to {file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching sequence for {protein_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_and_save_protein_sequence_from_UniProt(protein_ids, output_folder):\n",
    "    base_url = \"https://www.uniprot.org/uniprot/\"\n",
    "    headers = {\"Content-Type\": \"text/plain\"}\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # List to keep track of filenames for zipping\n",
    "    file_paths = []\n",
    "\n",
    "    # Iterate over protein IDs and fetch sequence in FASTA format\n",
    "    for item in protein_ids:\n",
    "        # get UniProt ID\n",
    "        protein_id = item\n",
    "\n",
    "        if protein_id != \"Not Found\" and protein_id != \"\":\n",
    "\n",
    "          # Construct the URL for the given protein ID\n",
    "          url = f\"{base_url}{protein_id}.fasta\"\n",
    "          response = requests.get(url, headers=headers)\n",
    "          file_path = os.path.join(output_folder, f\"{protein_id}.txt\")\n",
    "          file_paths.append(file_path)\n",
    "\n",
    "          # Save the FASTA sequence to the file\n",
    "          with open(file_path, \"w\") as file:\n",
    "             file.write(response.text)\n",
    "             print(f\"Saved FASTA sequence for {protein_id} to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "#Main\n",
    "# Load the CSV files into dataframes\n",
    "file1 = \"./animal_uniprot_id.csv\"  \n",
    "file2 = \"./human_uniprot_id.csv\"  \n",
    "file3 = \"./Uniport_merged_file.csv\" \n",
    "\n",
    "# Assuming tab-delimited files\n",
    "df_Animal_ID = pd.read_csv(file1)\n",
    "df_Human_ID = pd.read_csv(file2)\n",
    "df_NCBI_ID = pd.read_csv(file3) \n",
    "\n",
    "\n",
    "human_ids = df_Human_ID['Human ID'].tolist()\n",
    "animal_ids = df_Animal_ID['Animal ID'].tolist()\n",
    "NCBI_ids = df_NCBI_ID['NCBI ID'].tolist()\n",
    "NCBI_ids = list(set(NCBI_ids)) # ensure the contents of a Python list are unique\n",
    "\n",
    "#Fetch fasta file fron UniProt\n",
    "fetch_and_save_protein_sequence_from_UniProt(human_ids, \"Human_Uniprot\")\n",
    "fetch_and_save_protein_sequence_from_UniProt(animal_ids, \"Animal_Uniprot\")\n",
    "fetch_and_save_protein_sequence_from_ncbi(NCBI_ids, \"Animal_NCBI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify by search mutant in animal sequence\n",
    "#some records have no NCBI,no Uniprot\n",
    "#mutant may not match sequence\n",
    "#this records will be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9wBdZeS3dnJ",
    "outputId": "25a77978-015b-476e-ebf3-bb35c0defde4"
   },
   "outputs": [],
   "source": [
    "# partial Alignment and calculate similarity percentage\n",
    "\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from Bio.Align import PairwiseAligner\n",
    "import re\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "\n",
    "# Function to read FASTA file and return sequence\n",
    "def read_fasta(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        seq_record = SeqIO.read(file, \"fasta\")\n",
    "        return str(seq_record.seq)\n",
    "\n",
    "# Function to calculate similarity percentage\n",
    "def calculate_similarity(aligned_seq1, aligned_seq2):\n",
    "    matches = sum(1 for a, b in zip(aligned_seq1, aligned_seq2) if a == b)\n",
    "    total = len(aligned_seq1)\n",
    "    return (matches / total) * 100\n",
    "\n",
    "\n",
    "# Function to calculate similarity percentage for specified fragments\n",
    "def calculate_fragment_similarity(aligned_seq1, aligned_seq2, position, window):\n",
    "    \"\"\"\n",
    "    Extracts a fragment of length 2*window centered at the specified position in the aligned sequences,\n",
    "    and calculates the similarity percentage.\n",
    "\n",
    "    Args:\n",
    "        aligned_seq1 (str): The first aligned protein sequence.\n",
    "        aligned_seq2 (str): The second aligned protein sequence.\n",
    "        position (int): The 1-based index of the amino acid around which the fragment is extracted.\n",
    "        window (int): The number of amino acids before and after the position to include (default is 20).\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity percentage between the fragments.\n",
    "    \"\"\"\n",
    "    # Convert 1-based position to 0-based index for slicing\n",
    "    index = position - 1\n",
    "\n",
    "    # Extract the fragments around the specified position\n",
    "    start = max(0, index - window)\n",
    "    end = index + window + 1\n",
    "\n",
    "    fragment1 = aligned_seq1[start:end]\n",
    "    fragment2 = aligned_seq2[start:end]\n",
    "\n",
    "    # Log a message if the fragment is shorter than expected\n",
    "    if len(fragment1) < 2 * window + 1 or len(fragment2) < 2 * window + 1:\n",
    "        print(f\"Warning: Fragment length is shorter than expected at position {position}.\\n\"\n",
    "              f\"Fragment1 length: {len(fragment1)}, Fragment2 length: {len(fragment2)}\")\n",
    "\n",
    "    # Calculate similarity percentage\n",
    "    matches = sum(1 for a, b in zip(fragment1, fragment2) if a == b)\n",
    "    total = len(fragment1)\n",
    "    similarity = (matches / total) * 100\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def extract_and_convert(mutation_string):\n",
    "  \"\"\"Extracts the numerical part of a mutation string and converts it to an integer.\n",
    "\n",
    "  Args:\n",
    "    mutation_string: The input mutation string (e.g., \"R301C\").\n",
    "\n",
    "  Returns:\n",
    "    The numerical part as an integer, e.g., 301, or None if not found.\n",
    "  \"\"\"\n",
    "  match = re.search(r\"(\\d+)\", mutation_string)  # Find the numerical part\n",
    "  if match:\n",
    "    print( int(match.group(1)))\n",
    "    return int(match.group(1))  # Convert to integer\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "\n",
    "def check_mutation(sequence, mutation):\n",
    "    \"\"\"\n",
    "    Checks if the specified amino acid mutation matches the sequence in the FASTA file.\n",
    "\n",
    "    :param fasta_file: Path to the FASTA file.\n",
    "    :param mutation: Mutation string in the format 'M200T'.\n",
    "    :return: True if the amino acid matches, otherwise False.\n",
    "    \"\"\"\n",
    "    # Parse the mutation\n",
    "    original_aa = mutation[0]\n",
    "    position = int(mutation[1:-1])\n",
    "    mutated_aa = mutation[-1]\n",
    "\n",
    "\n",
    "    # Check if the amino acid at the specified position matches\n",
    "    if position <= len(sequence) and sequence[position - 1] == original_aa:\n",
    "        print(f\"Position {position} in the sequence matches the original amino acid '{original_aa}'.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Mismatch: Position {position} does not have the amino acid '{original_aa}' in the sequence.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# Function to format sequences with highlighted positions\n",
    "# Problem: it seems some animal mutants do not match corresponding NCBI sequence, in this case, will try sequence form UniProt, if fails again, return Non\n",
    "# it there is no match human amino acid, rerurn -1 and skip without output\n",
    "def format_sequence_with_positions(human_sequence, horse_sequence, animal_mutant):\n",
    "    \"\"\"Formats sequences with residue numbering and highlights multiple residues in both sequences.\n",
    "\n",
    "    Args:\n",
    "        human_sequence: The aligned human protein sequence.\n",
    "        horse_sequence: The aligned horse protein sequence.\n",
    "        horse_residue_positions: A list of positions to highlight in the horse sequence.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string with residue numbering and highlighting.\n",
    "    \"\"\"\n",
    "    formatted = \"\"\n",
    "    human_residue_number = 1\n",
    "    horse_residue_number = 1\n",
    "    horse_mutant = \"\"\n",
    "    humnan_mutant = \"\"\n",
    "    alignment_sum = \"\"\n",
    "\n",
    "\n",
    "    #print(animal_mutant) pass\n",
    "    # R301C to 301\n",
    "    horse_residue_positions = extract_and_convert(animal_mutant)   # Convert to integers\n",
    "\n",
    "\n",
    "    for i in range(0, len(human_sequence), 100):\n",
    "        human_line = human_sequence[i:i+100]\n",
    "        horse_line = horse_sequence[i:i+100]\n",
    "\n",
    "        human_line_numbered = f\"{human_residue_number:>5} \"\n",
    "        horse_line_numbered = f\"{horse_residue_number:>5} \"\n",
    "        match_line = \" \" * 5\n",
    "\n",
    "        for h_residue, hs_residue in zip(human_line, horse_line):\n",
    "            \n",
    "            if h_residue != '-':\n",
    "                # Highlight corresponding human residues with []\n",
    "                if horse_residue_number == horse_residue_positions:\n",
    "                    human_line_numbered += f\"[{h_residue}]\"\n",
    "                    humnan_mutant = h_residue # save\n",
    "                else:\n",
    "                    human_line_numbered += h_residue\n",
    "                human_residue_number += 1\n",
    "            else:\n",
    "                # no matching human residue\n",
    "                human_line_numbered += \" \"\n",
    "                #print(f\"No matching human residue with mutant {animal_mutant}\")\n",
    "                #result_file.write(f\"{animal} {gene} | No matching human residue found for mutant {animal_mutant} {e}\\n\")\n",
    "                #continue #skip to next record\n",
    "\n",
    "            if hs_residue != '-':\n",
    "                # Highlight specified horse residues with []\n",
    "                if horse_residue_number == horse_residue_positions:\n",
    "                    horse_line_numbered += f\"[{hs_residue}]\"\n",
    "                    horse_mutant = hs_residue # save\n",
    "                    human_pos = find_corresponding_human_pos(human_sequence, horse_sequence, horse_residue_number)\n",
    "                    formatted += f\"Animal residue position {horse_mutant}{horse_residue_number} corresponds to Human residue position {humnan_mutant}{human_pos}\\n\"\n",
    "                    alignment_sum += f\"Animal mutant {animal_mutant} : Human mutant {humnan_mutant}{human_pos}{animal_mutant[-1]}\\n\"\n",
    "                else:\n",
    "                    horse_line_numbered += hs_residue\n",
    "                horse_residue_number += 1\n",
    "            else:\n",
    "                horse_line_numbered += \" \"\n",
    "\n",
    "            match_line += \"*\" if h_residue == hs_residue and h_residue != '-' else \" \"\n",
    "\n",
    "        formatted += f\"Animal:  {horse_line_numbered}\\n\"\n",
    "        formatted += f\"Human:   {human_line_numbered}\\n\"\n",
    "        formatted += f\"        {match_line}\\n\\n\"\n",
    "\n",
    "    formatted += f\"{alignment_sum}\\n\"\n",
    "    return formatted\n",
    "\n",
    "# Function to find corresponding human residue position\n",
    "def find_corresponding_human_pos(human_seq, horse_seq, horse_pos):\n",
    "    \"\"\"Finds the corresponding human residue position for a given Animal residue position.\"\"\"\n",
    "    horse_count = 0\n",
    "    human_count = 0\n",
    "\n",
    "    for i in range(len(horse_seq)):\n",
    "        if horse_count == horse_pos:\n",
    "            return human_count\n",
    "\n",
    "        if horse_seq[i] != '-':\n",
    "            horse_count += 1\n",
    "        if human_seq[i] != '-':\n",
    "            human_count += 1\n",
    "\n",
    "    return -1  # If not found (shouldn't happen in normal cases)\n",
    "\n",
    "# Main function to align and generate report\n",
    "def align_and_generate_report(merged_all_data, output_file=\"alignment_results.txt\", human_data_dir=\"./Human_Uniprot\", horse_data_dir=\"./Animal_Uniprot\",horse_data_NCBI_dir=\"./Animal_NCBI\"):\n",
    "\n",
    "    aligner = PairwiseAligner()\n",
    "\n",
    "    with open(output_file, \"w\") as result_file:\n",
    "        result_file.write(\"Human ID | Species Name | Protein Name | Human Sequence | Animal Sequence | Similarity (%)\\n\")\n",
    "        result_file.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "        for index, row in merged_all_data.iterrows():\n",
    "             animal = row['Species Name']\n",
    "             gene = row['Gene']\n",
    "             pathog = row['Deleterious?']\n",
    "             human_protein_id = row['Human ID']\n",
    "             horse_protein_id = row['Animal ID']\n",
    "             horse_NCBI_id = row['NCBI ID']\n",
    "             animal_mutant = row['Animal Mutant']\n",
    "\n",
    "             print(animal, gene, animal_mutant)\n",
    "\n",
    "             try:\n",
    "                # read human sequence \n",
    "                human_file_path = os.path.join(human_data_dir, f\"{human_protein_id}.txt\")\n",
    "                if not os.path.exists(human_file_path):\n",
    "                  raise FileNotFoundError(f\"Warning: Human Sequence File not found: {human_file_path}\")\n",
    "                else:\n",
    "                  human_seq = read_fasta(human_file_path)\n",
    "\n",
    "                # read animal sequence, uniprot\n",
    "                horse_file_path = os.path.join(horse_data_dir, f\"{horse_protein_id}.txt\")\n",
    "                if not os.path.exists(horse_file_path):\n",
    "                   print(f\"Warning: Animal Sequence (UniProt) File not found: {horse_file_path}\")\n",
    "                   horse_seq = \"\"\n",
    "                else:\n",
    "                  horse_seq = read_fasta(horse_file_path)\n",
    "\n",
    "                # read animal sequence, NCBI\n",
    "                horse_NCBI_file_path = os.path.join(horse_data_NCBI_dir, f\"{horse_NCBI_id}.txt\")\n",
    "                if not os.path.exists(horse_NCBI_file_path):\n",
    "                   print(f\"Warning: Animal Sequence (NCBI) File not found: {horse_NCBI_file_path}\")\n",
    "                   horse_NCBI_seq = \"\"\n",
    "                else:\n",
    "                  horse_NCBI_seq = read_fasta(horse_NCBI_file_path)\n",
    "\n",
    "\n",
    "                #check which animal sequence match the mutant\n",
    "                if check_mutation(horse_NCBI_seq, animal_mutant):\n",
    "                  print(\"Using NCBI Sequence File for alignment!\")\n",
    "                  alignment = aligner.align(human_seq, horse_NCBI_seq)\n",
    "                else:\n",
    "                  if check_mutation(horse_seq, animal_mutant):\n",
    "                    print(\"Using UniProt Sequence File for alignment!\")\n",
    "                    alignment = aligner.align(human_seq, horse_seq)\n",
    "                  else:\n",
    "                    print(f\"No Sequence File matchs {gene} with mutant {animal_mutant}\")\n",
    "                    result_file.write(f\"{animal} {gene} | No matching sequence found for mutant {animal_mutant} due to no matching animal sequence {e}\\n\")\n",
    "                    continue #skip to next record\n",
    "\n",
    "\n",
    "                aligned_human = str(alignment[0][0])\n",
    "                aligned_horse = str(alignment[0][1])\n",
    "\n",
    "                similarity = calculate_similarity(aligned_human, aligned_horse)\n",
    "                 \n",
    "                horse_residue_positions = extract_and_convert(animal_mutant)   #   Specify the 1-based position of the amino acid\n",
    "                similarity20 = calculate_fragment_similarity(aligned_horse, aligned_human, horse_residue_positions,10)\n",
    "                similarity40 = calculate_fragment_similarity(aligned_horse, aligned_human, horse_residue_positions,20)\n",
    "                similarity60 = calculate_fragment_similarity(aligned_horse, aligned_human, horse_residue_positions,30)\n",
    "                similarity80 = calculate_fragment_similarity(aligned_horse, aligned_human, horse_residue_positions,40)\n",
    "                #print(similarity) pass\n",
    "\n",
    "                #print(horse_residue_positions) pass\n",
    "\n",
    "                formatted_sequences = format_sequence_with_positions(aligned_human, aligned_horse, animal_mutant)\n",
    "                result_file.write(f\"{human_protein_id} |\") \n",
    "                result_file.write(f\"{animal} |\")\n",
    "                result_file.write(f\"{pathog} |\")\n",
    "                result_file.write(f\"{gene} |\\n{formatted_sequences}\\n\")\n",
    "                result_file.write(f\"Similarity: {similarity:.2f}%\\n\")\n",
    "                result_file.write(f\"Similarity20: {similarity20:.2f}%\\n\")\n",
    "                result_file.write(f\"Similarity40: {similarity40:.2f}%\\n\")\n",
    "                result_file.write(f\"Similarity60: {similarity60:.2f}%\\n\")\n",
    "                result_file.write(f\"Similarity80: {similarity80:.2f}%\\n\")\n",
    "                result_file.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "                print(f\"Alignment for {gene} completed and written to file.\")\n",
    "             except Exception as e:\n",
    "                print(f\"Error processing {gene}: {e}\")\n",
    "                result_file.write(f\"{gene} | Error: {e}\\n\")\n",
    "                result_file.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Load merged_all_data\n",
    "all_data = pd.read_csv(\"./Uniport_merged_file.csv\")\n",
    "\n",
    "# Run the main function\n",
    "align_and_generate_report(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-zKakcGtvN_",
    "outputId": "eb936494-91f0-4abe-c28b-b62ba7650e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment summary written to alignment_summary.txt\n"
     ]
    }
   ],
   "source": [
    "def summarize_alignment(input_file, output_file):\n",
    "  \"\"\"Reads an alignment file, removes lines starting with \"Animal:\", \"Human:\", \"Horse residue position\",\n",
    "  containing \"*\", and blank lines, and writes the remaining lines to a new file.\n",
    "\n",
    "  Args:\n",
    "    input_file: Path to the input alignment file.\n",
    "    output_file: Path to the output summary file.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            if not line.startswith(('Animal:', 'Human:', 'Animal residue position')) and \"*\" not in line and line.strip():\n",
    "                outfile.write(line)\n",
    "    print(f\"Alignment summary written to {output_file}\")\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{input_file}' not found.\")\n",
    "  except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = \"alignment_results.txt\"\n",
    "output_file = \"alignment_summary.txt\"\n",
    "summarize_alignment(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgNePK3EsMid",
    "outputId": "346316f9-0722-4ea1-9ec7-8268f008219c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine all data for prediction\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file\n",
    "file_path = \"alignment_summary.txt\"  # Replace with your file path\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize lists to hold data\n",
    "human_id = []\n",
    "animal = []\n",
    "pathog = []\n",
    "Gene = []\n",
    "Animal_mutant = []\n",
    "Human_mutant = []\n",
    "similarities = []\n",
    "similarities20 = []\n",
    "similarities40 = []\n",
    "similarities60 = []\n",
    "similarities80 = []\n",
    "\n",
    "# Parse the lines, skipping the first line (header)\n",
    "for line in lines[1:]:  # Start from the second line\n",
    "    line = line.strip()\n",
    "    if line.endswith(\"|\") and not line.startswith(\"=\"):\n",
    "        human_id.append(line.split(\" |\")[0].strip())\n",
    "        animal.append(line.split(\" |\")[1].strip())\n",
    "        pathog.append(line.split(\" |\")[2].strip())\n",
    "        Gene.append(line.split(\" |\")[3].strip())\n",
    "        # Ensure all other lists grow as well\n",
    "        Animal_mutant.append(None)\n",
    "        Human_mutant.append(None)\n",
    "        similarities.append(None)\n",
    "        similarities20.append(None)\n",
    "        similarities40.append(None)\n",
    "        similarities60.append(None)\n",
    "        similarities80.append(None)\n",
    "    elif \"Animal mutant\" in line:\n",
    "        parts = line.split(\":\")\n",
    "        Animal_mutant[-1] = parts[0].split(\"Animal mutant\")[-1].strip()\n",
    "        Human_mutant[-1] = parts[1].split(\"Human mutant\")[-1].strip()\n",
    "    elif \"Similarity\" in line and \"Similarity20\" not in line and \"Similarity40\" not in line and \"Similarity60\" not in line and \"Similarity80\" not in line:\n",
    "        similarities[-1] = float(line.split(\":\")[-1].strip().replace(\"%\", \"\"))\n",
    "    elif \"Similarity20\" in line:\n",
    "        similarities20[-1] = float(line.split(\":\")[-1].strip().replace(\"%\", \"\"))\n",
    "    elif \"Similarity40\" in line:\n",
    "        similarities40[-1] = float(line.split(\":\")[-1].strip().replace(\"%\", \"\"))\n",
    "    elif \"Similarity60\" in line:\n",
    "        similarities60[-1] = float(line.split(\":\")[-1].strip().replace(\"%\", \"\"))\n",
    "    elif \"Similarity80\" in line:\n",
    "        similarities80[-1] = float(line.split(\":\")[-1].strip().replace(\"%\", \"\"))\n",
    "\n",
    "# Create the DataFrame\n",
    "data = {\n",
    "    \"Human ID\":human_id,\n",
    "    \"Species Name\":animal,\n",
    "    \"Gene\": Gene,\n",
    "    \"Animal Mutant\": Animal_mutant,\n",
    "    \"'Deleterious?\":pathog,\n",
    "    \"Human Mutant\": Human_mutant,\n",
    "    \"Similarity (%)\": similarities,\n",
    "    \"Similarity20 (%)\": similarities20,\n",
    "    \"Similarity40 (%)\": similarities40,\n",
    "    \"Similarity60 (%)\": similarities60,\n",
    "    \"Similarity80 (%)\": similarities80,\n",
    "}\n",
    "\n",
    "#print(data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Label records which have no matching human amino acid\n",
    "df['Label'] = df['Human Mutant'].apply(lambda x: \"No Matching\" if x[0].isdigit() else \"\")\n",
    "\n",
    "print(df)\n",
    " \n",
    "#save All_data as csv file\n",
    "df.to_csv('final_all_data.csv')\n",
    "print(\"Final data saved to final_all_data.csv\")\n",
    "\n",
    "\n",
    "# Discard rows where the \"Label\" column is \"No Matching\"\n",
    "df_filtered = df[df['Label'] != 'No Matching']\n",
    "# save work data\n",
    "df_filtered.to_csv('work_all_data.csv')\n",
    "print(\"Working data saved to work_all_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umdSEC96rqs6"
   },
   "source": [
    "            ####################################################################################\n",
    "            ####################################################################################\n",
    "            ####################################################################################\n",
    "                            Follwing code is for ESM prediction\n",
    "            ####################################################################################\n",
    "            ####################################################################################\n",
    "            ####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "X3cQmlEXrxvK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected ESM LLR files have been copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "# selected needed LLR files by Human UniProt ID and save at ./selected_LLR\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory_path = '../ESM1b/LLR/content'\n",
    "\n",
    "# Path to the destination directory where selected files will be copied\n",
    "destination_path = '../ESM1b/selected_LLR'\n",
    "\n",
    "# Read the list CSV file\n",
    "list_df = pd.read_csv('./human_uniprot_id.csv')  # Adjust the path to the uploaded file\n",
    "\n",
    "# Extract the UniProt codes from the list\n",
    "uniprot_id = list_df['Human ID'].tolist()\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(destination_path):\n",
    "    os.makedirs(destination_path)\n",
    "\n",
    "# Iterate through the directory and select files\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Check if the filename matches the exact pattern '<uniprot_id>_LLR.csv'\n",
    "    if any(filename == f\"{id}_LLR.csv\" for id in uniprot_id):\n",
    "        source_file = os.path.join(directory_path, filename)\n",
    "        destination_file = os.path.join(destination_path, filename)\n",
    "        shutil.copyfile(source_file, destination_file)\n",
    "\n",
    "print(\"Selected ESM LLR files have been copied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7k5GOnSqsILD"
   },
   "outputs": [],
   "source": [
    "# transfer original ESM LLR data to format of AlphaMisence\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# \tM 1\tG 2\n",
    "# K\t-14.012\t-6.545\n",
    "# R\t-11.862\t-5.078\n",
    "# H\t-14.163\t-7.058\n",
    "# E\t-12.91\t-5.521\n",
    "# to\n",
    "# M1K\t-14.012\n",
    "# G2K\t-6.545\n",
    "# S3K\t-6.324\n",
    "# R4K\t-6.808\n",
    "# C5K\t-3.984\n",
    "# A6K\t-8.685\n",
    "# L7K\t-9.151\n",
    "# transfer all selected LLR files to new data format\n",
    "\n",
    "directory_path = '../ESM1b/selected_LLR'\n",
    "\n",
    "# Path to the destination directory where selected files will be copied\n",
    "output_directory = '../ESM1b/selected_transferred_LLR'\n",
    "\n",
    "# Read the list CSV file\n",
    "list_df = pd.read_csv('./human_uniprot_id.csv') \n",
    "\n",
    "# Extract the UniProt codes from the list\n",
    "uniprot_id = list_df['Human ID'].tolist()\n",
    "\n",
    "# Iterate through the directory and select files\n",
    "for filename in uniprot_id:\n",
    "    source_file = os.path.join(directory_path, filename + '_LLR.csv')\n",
    "\n",
    "    if os.path.exists(source_file):\n",
    "\n",
    "        df = pd.read_csv(source_file, index_col=0)\n",
    "\n",
    "        # Create a list to store the new rows\n",
    "        new_rows = []\n",
    "\n",
    "        # Iterate through the dataframe and construct the new rows\n",
    "        for index, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                new_row = {'Key': f\"{col}{index}\", 'Value': row[col]}\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "        # Create a new dataframe from the new rows\n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "        # Remove all blanks from the 'Key' column\n",
    "        new_df['Key'] = new_df['Key'].str.replace(' ', '')\n",
    "\n",
    "        # Save the new dataframe to a CSV file\n",
    "        output_file = os.path.join(output_directory, filename + '_LLR.csv' )\n",
    "        new_df.to_csv(output_file, index=False, header=False)\n",
    "\n",
    "        print(filename + '_LLR.csv' + \" has been created successfully.\")\n",
    "    else:\n",
    "        print(filename + '_LLR.csv' + ' not exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "u1YVc_GWsfqS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ESM Score\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# search each mutant in LLR file and get score\n",
    "# Step 1: Read the input CSV file\n",
    "\n",
    "df1 = pd.read_csv('work_all_data.csv')  # Adjust the path to the uploaded file\n",
    "\n",
    "\n",
    "# Folder path for the alpha CSV files\n",
    "folder_path = '../ESM1b/selected_transferred_LLR'\n",
    "\n",
    "# Columns to append\n",
    "df1['ESM_Score'] = None\n",
    "\n",
    "# Iterate through df1 rows\n",
    "for index, row in df1.iterrows():\n",
    "    human_id = row['Human ID']\n",
    "    human_mutant = row['Human Mutant']\n",
    "\n",
    "    # Construct file path for the corresponding CSV file\n",
    "    filename = human_id + '_LLR.csv'\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the CSV file\n",
    "        csv_data = pd.read_csv(file_path, header=None)\n",
    "        headers = ['Mutant', 'ESM_Value']\n",
    "        csv_data.columns = headers\n",
    "        #print (csv_data)\n",
    "        # Search for the Human Mutant in the protein_variant column\n",
    "        match = csv_data[csv_data['Mutant'] == human_mutant]\n",
    "        #print (match)\n",
    "        if not match.empty:\n",
    "            # Retrieve am_pathogenicity and am_class values\n",
    "            df1.at[index, 'ESM_Score'] = match['ESM_Value'].values[0]\n",
    "\n",
    "# Save the updated df1 to a new file\n",
    "df1.to_csv(\"work_all_data_ESM.csv\", index=False)\n",
    "\n",
    "print(\"Done ESM Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clc0Icmys9jz"
   },
   "source": [
    "            ####################################################################################\n",
    "            ####################################################################################\n",
    "            ####################################################################################\n",
    "                    The following code is for AlphaMisense prediction\n",
    "            ####################################################################################\n",
    "            ####################################################################################\n",
    "            ####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibg6uIm_sK92"
   },
   "outputs": [],
   "source": [
    "#get alphamissence scroe data in csv format, each human protein with one csv file\n",
    "#https://alphafold.ebi.ac.uk/files/AF-Q01726-F1-hg19.csv\n",
    "#https://alphafold.ebi.ac.uk/files/AF-Q01726-F1-hg38.csv\n",
    "# get alphamissence score data from alphafold.ebi.ac.uk\n",
    "#https://alphafold.ebi.ac.uk/files/AF-Q9H2P0-F1-aa-substitutions.csv\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "All_data_df = pd.read_csv(\"./human_uniprot_id.csv\")\n",
    "\n",
    "# List of gene IDs\n",
    "gene_ids = All_data_df['Human ID'].tolist() #    [\"Q01726\", \"Q04526\"]   Add your gene IDs here\n",
    "\n",
    "# Folder to save the downloaded files\n",
    "output_folder = \"../AlphaMissense/Alpha_substitutions\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Base URL for downloading files\n",
    "base_url = \"https://alphafold.ebi.ac.uk/files/\"\n",
    "\n",
    "# Download files with a 0.1-second delay\n",
    "for gene_id in gene_ids:\n",
    "    #file_name = f\"AF-{gene_id}-F1-hg38.csv\"\n",
    "    #https://alphafold.ebi.ac.uk/files/AF-Q9H2P0-F1-aa-substitutions.csv\n",
    "    file_name = f\"AF-{gene_id}-F1-aa-substitutions.csv\"\n",
    "    url = f\"{base_url}{file_name}\"\n",
    "    output_file = os.path.join(output_folder, f\"{gene_id}.csv\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for failed requests\n",
    "\n",
    "        # Save the file with the new name\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Saved: {output_file}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "    # Wait for 0.05 second before sending the next request\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BeHzb1KttHaT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated df1 saved as 'work_all_data_alphamissense.csv'.\n"
     ]
    }
   ],
   "source": [
    "#alphamissence score predict\n",
    "# All data from alignment\n",
    "#\tHuman ID\tSpecies Name\tGene\tAnimal Mutant\tHuman Mutant\tSimilarity (%)\tSimilarity20 (%)\tSimilarity40 (%)\tSimilarity60 (%)\tSimilarity80 (%)\tLabel\n",
    "#0\tQ9UPW5      sheep\t       AGTPBP1\t    R970P\t      R970P\t           91.19\t         100\t            95.12\t           96.72\t             97.53\t\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the work data, 15 redords without matching human residues have beed discarded \n",
    "df1 = pd.read_csv(\"./work_all_data.csv\")\n",
    "\n",
    "# Folder path for the alpha CSV files\n",
    "folder_path = \"../AlphaMissense/Alpha_substitutions\"\n",
    "\n",
    "# Columns to append\n",
    "df1['am_pathogenicity'] = None\n",
    "df1['am_class'] = None\n",
    "\n",
    "# Iterate through df1 rows\n",
    "for index, row in df1.iterrows():\n",
    "    human_id = row['Human ID']\n",
    "    human_mutant = row['Human Mutant']\n",
    "\n",
    "    # Construct file path for the corresponding CSV file\n",
    "    file_path = os.path.join(folder_path, f\"{human_id}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the CSV file\n",
    "        csv_data = pd.read_csv(file_path)\n",
    "\n",
    "        # Search for the Human Mutant in the protein_variant column\n",
    "        match = csv_data[csv_data['protein_variant'] == human_mutant]\n",
    "\n",
    "        if not match.empty:\n",
    "            # Retrieve am_pathogenicity and am_class values\n",
    "            df1.at[index, 'am_pathogenicity'] = match['am_pathogenicity'].values[0]\n",
    "            df1.at[index, 'am_class'] = match['am_class'].values[0]\n",
    "\n",
    "# Save the updated df1 to a new file\n",
    "df1.to_csv(\"work_all_data_alphamissense.csv\", index=False)\n",
    "\n",
    "print(\"Updated df1 saved as 'work_all_data_alphamissense.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PolyPhen-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to ./polyphen_input.txt in polyphen-2 input format\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# prepare polyphen-2 input data\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"./work_all_data.csv\" \n",
    "output_file = './polyphen_input.txt'\n",
    "\n",
    "# Open the CSV file and read its contents\n",
    "with open(input_file, mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    # Open the output file in write mode\n",
    "    with open(output_file, mode='w') as txt_file:\n",
    "        for row in csv_reader:\n",
    "            human_id = row['Human ID']\n",
    "            position = ''.join(filter(str.isdigit, row['Human Mutant']))\n",
    "            original = row['Human Mutant'][0]\n",
    "            mutant = row['Human Mutant'][-1]\n",
    "            txt_file.write(f\"{human_id} {position} {original} {mutant}\\n\")\n",
    "\n",
    "print(f\"Data successfully written to {output_file} in polyphen-2 input format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer polyphen-2 result to csv format \n",
    "\n",
    "import csv\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = './pph2-short.txt'  # Replace with your TXT file path\n",
    "output_file = './polyphen2_output.csv'\n",
    "\n",
    "# Open the TXT file and read its contents\n",
    "with open(input_file, mode='r') as txt_file:\n",
    "    # Read and clean the header\n",
    "    headers = txt_file.readline().strip().split('\\t')\n",
    "    headers = [header.strip() for header in headers]  # Remove extra spaces\n",
    "\n",
    "    # Debugging: Print the headers to verify\n",
    "    print(\"Headers in TXT file:\", headers)\n",
    "\n",
    "    # Identify the indexes of needed columns\n",
    "    columns_to_keep = ['#o_acc', 'o_pos', 'o_aa1', 'o_aa2', 'prediction', 'pph2_prob']\n",
    "    try:\n",
    "        column_indexes = [headers.index(col) for col in columns_to_keep]\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: One or more columns from {columns_to_keep} are missing in the file headers.\")\n",
    "        raise e\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # Write the header row\n",
    "        csv_writer.writerow(['Human ID', 'Human Mutant', 'Prediction', 'PPH2_Prob'])\n",
    "\n",
    "        # Process each line in the TXT file\n",
    "        for line_number, line in enumerate(txt_file, start=2):  # Start with line 2 for debugging\n",
    "            row = line.strip().split('\\t')\n",
    "\n",
    "            # Skip rows with insufficient columns\n",
    "            if len(row) < len(headers):\n",
    "                print(f\"Warning: Row {line_number} has fewer columns than expected. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Extract data using column indexes\n",
    "                human_id = row[column_indexes[0]]\n",
    "                position = row[column_indexes[1]]\n",
    "                aa1 = row[column_indexes[2]]\n",
    "                aa2 = row[column_indexes[3]]\n",
    "                prediction = row[column_indexes[4]]\n",
    "                pph2_prob = row[column_indexes[5]]\n",
    "\n",
    "                # Create the new Human Mutant column in R970P format\n",
    "                human_mutant = f\"{aa1}{position}{aa2}\"\n",
    "\n",
    "                #clean\n",
    "                human_id = human_id.replace(\" \", \"\")\n",
    "                human_mutant = human_mutant.replace(\" \", \"\")\n",
    "\n",
    "                # Write the processed row to the CSV\n",
    "                csv_writer.writerow([human_id, human_mutant, prediction, pph2_prob])\n",
    "            except IndexError as e:\n",
    "                print(f\"Error: Issue processing row {line_number}. Row data: {row}\")\n",
    "                raise e\n",
    "\n",
    "print(f\"Data successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as ./work_all_data_PolyPhen.csv\n"
     ]
    }
   ],
   "source": [
    "# merge polyphen2 output with working data\n",
    "import pandas as pd\n",
    "\n",
    "# Read the first CSV file\n",
    "file1 = \"./work_all_data.csv\"    \n",
    "df1 = pd.read_csv(file1)\n",
    "\n",
    "# Read the second CSV file\n",
    "file2 = './polyphen2_output.csv'  \n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# Merge the two dataframes on 'Human ID' and 'Human Mutant'\n",
    "merged_df = pd.merge(df1, df2, on=['Human ID', 'Human Mutant'], how='left')\n",
    "\n",
    "# Save the merged result to a new CSV file\n",
    "output_file = \"./work_all_data_PolyPhen.csv\"    \n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged file saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
